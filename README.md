# ğŸ§  Offer Letter Generator - Agentic System

This repository contains an **Agentic system** to generate offer letters for candidates based on salary breakup and HR policies applicable to their function, team, and salary band. Built with modular components, it integrates document parsing, embedding storage, RAG pipelines, LLM-based generation, Jinja2 fallbacks, and a Streamlit frontend.

---

## ğŸ¯ Project Overview

**Objective:**  
Automate creation of formal offer letters by combining company policies and candidate metadata through a retrieval-augmented generation (RAG) pipeline, with fallback templating for reliability.

![System Architecture](assets/dashboard.jpeg)
![Video Drive Link](https://drive.google.com/drive/folders/1jtBHUMfHAq8TfKRY9fJ209MYhXAgU98c?usp=sharing)

---

## âœ¨ Key Features

- ğŸ“„ **Document Parsing**  
  Intelligent chunking of HR PDFs and company policies.

- ğŸ” **Vector Embeddings**  
  Embedding generation and storage using **Qdrant** vector database.

- ğŸ¤– **Contextual Retriever**  
  Retrieves relevant policy context based on candidate metadata.

- ğŸ§  **LLM-based RAG Generation**  
  Uses GPT to generate personalized offer letters using contextual information.

- ğŸ”§ **Jinja2 Templating Fallback**  
  Provides reliable fallback in case LLM generation fails or is toggled off.

- âš¡ **FastAPI Backend**  
  Serves generation endpoints as modular APIs.

- ğŸŒ **Streamlit Frontend**  
  Includes a user-friendly interface for HR users to generate letters.

- ğŸ“‹ **PDF Export**  
  Generates offer letters in downloadable PDF format with Unicode font support.

- ğŸ”„ **Toggle Modes**  
  Allows switching between GPT-based and template-based generation dynamically.

---

## ğŸ“ Tech Stack Summary

- **Backend:** Python, FastAPI, Jinja2
- **Frontend:** Streamlit, HTML/CSS
- **LLM Integration:** OpenAI GPT
- **Vector DB:** Qdrant
- **PDF Generation:** WeasyPrint / ReportLab (Unicode support)
- **Document Parsing:** PyMuPDF, custom chunking logic

---

## ğŸ—‚ï¸ Directory Structure

```
project-root/
â”œâ”€â”€ data/                         # ğŸ“¦ All input/output data
â”‚   â”œâ”€â”€ raw_pdfs/                 # HR policies and sample letters
â”‚   â”‚   â”œâ”€â”€ HR Leave Policy.pdf
â”‚   â”‚   â”œâ”€â”€ HR Travel Policy.pdf
â”‚   â”‚   â””â”€â”€ HR Offer Letter.pdf
â”‚   â”œâ”€â”€ docs_chunks/             # Chunked JSONs from PDFs
â”‚   â”œâ”€â”€ embeddings/              # Embeddings (raw)
â”‚   â”œâ”€â”€ qdrant_ready_embeddings/ # Qdrant-compatible embeddings
â”‚   â”œâ”€â”€ employee_list.csv        # Source employee metadata
â”‚   â”œâ”€â”€ employee_list.json       # Converted JSON
â”‚   â”œâ”€â”€ wfo_policy.json          # Mapping of team to WFO policy
â”‚   â”œâ”€â”€ generated_letters/       # Markdown/Plaintext outputs
â”‚   â””â”€â”€ offer_letters/           # Final offer letter PDFs
â”‚
â”œâ”€â”€ backend/                     # ğŸ§  Core logic + model pipeline
â”‚   â”œâ”€â”€ ingest/                  # Chunking, embedding, upload
â”‚   â”‚   â”œâ”€â”€ chunk_and_embed.py
â”‚   â”‚   â””â”€â”€ upload_qdrant.py
â”‚   â”œâ”€â”€ retriever.py             # Qdrant-based retriever
â”‚   â”œâ”€â”€ generate_offer_letter.py # RAG-based generation (LLM + retriever)
â”‚   â”œâ”€â”€ generate_offer_withoutrag.py # LLM-only generator (no retrieval)
â”‚   â””â”€â”€ generate_offer_letter_nollm.py # Jinja2 fallback generator
â”‚
â”œâ”€â”€ utils/                       # ğŸ”§ Shared helpers/utilities
â”‚   â”œâ”€â”€ load_employee_metadata.py
â”‚   â””â”€â”€ save_offer_letter_pdf.py
â”‚
â”œâ”€â”€ templates/                   # ğŸ“„ Jinja2 fallback templates
â”‚   â””â”€â”€ offer_template.txt
â”‚
â”œâ”€â”€ frontend/                    # ğŸ›ï¸ UI layer
â”‚   â”œâ”€â”€ app.py                   # Streamlit UI
â”‚   â””â”€â”€ static_ui/               # (Optional) HTML-based UI
â”‚       â””â”€â”€ index.html
â”‚
â”œâ”€â”€ api/                         # ğŸŒ REST API
â”‚   â””â”€â”€ api_server.py            # FastAPI backend server
â”‚
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ chunking.log             # Chunking log
â”‚
â”œâ”€â”€ requirements.txt             # Python dependencies
â””â”€â”€ README.md                    # Project documentation

```
---
## Entire Workflow
```mermaid
flowchart LR
    %% Ingestion
    subgraph Ingestion_Indexing["ğŸ“¥ Ingestion & Indexing"]
        A["ğŸ“ Raw PDFs (HR Policies & Templates)"] --> B["ğŸ”ª Chunking <br>('unstructured' + heuristics)"]
        B --> C["ğŸ“¦ Chunks JSON <br>(docs_chunks/)"]
        C --> D["ğŸ§  Embedding Generation <br>(text-embedding-3-small)"]
        D --> E["ğŸ§© Embedding Vectors <br>(qdrant_ready_embeddings/)"]
        E --> F["ğŸ“¤ Qdrant Upload <br>('policy_chunks' collection)"]
    end

    %% Retrieval + Metadata
    subgraph Retrieval_Metadata["ğŸ” Retrieval + Metadata"]
        G["ğŸ‘¤ User Query: 'Generate for X'"] --> H["ğŸ“š Retriever <br>(retrieve_relevant_chunks)"]
        H --> I["ğŸ† Top-k Chunks"]
        J["ğŸ—‚ï¸ employee_list.json"] --> K["ğŸ“¥ load_employee_metadata"]
        K --> L["ğŸ‘¨â€ğŸ’¼ Employee Metadata Dict"]
    end

    %% Generation
    subgraph Generation_Pipeline["ğŸ§¾ Offer Letter Generation"]
        subgraph RAG["ğŸ¤– RAG (GPT-4o)"]
            I --> M["ğŸ“ generate_offer_letter.py <br>(LLM + Context)"]
            L --> M
            M --> N{"âœ… Success?"}
        end
        subgraph Fallback["ğŸ“„ Fallback (Jinja2)"]
            L --> O["ğŸ“„ generate_offer_letter_jinja.py"]
            I --> O
        end
        N -- "Yes" --> P["ğŸ“ƒ Offer Letter Text"]
        N -- "No" --> O
        O --> P
    end

    %% API Layer
    subgraph API_Layer["ğŸ”Œ API Interface"]
        P --> Q["ğŸŒ FastAPI Endpoint <br>/generate-offer-letter"]
    end

    %% Frontend
    subgraph Frontends["ğŸ–¥ï¸ Frontend Interfaces"]
        Q --> R1["ğŸ’¬ Static Chat UI <br>(index.html)"]
        Q --> R2["ğŸ›ï¸ Streamlit App <br>(app.py)"]
    end

    %% Output
    subgraph Output["ğŸ“¤ Output"]
        P --> S["ğŸ–¨ï¸ PDF Export <br>(save_offer_letter_pdf)"]
        S --> T["ğŸ“ Downloadable PDF"]
        R2 --> U["ğŸ•˜ Session History"]
    end

    %% Styling blocks for readability
    style Ingestion_Indexing fill:#f9f,stroke:#444,stroke-width:2px
    style Retrieval_Metadata fill:#bbf,stroke:#444,stroke-width:2px
    style Generation_Pipeline fill:#bfb,stroke:#444,stroke-width:2px
    style API_Layer fill:#ffd,stroke:#444,stroke-width:2px
    style Frontends fill:#fbf,stroke:#444,stroke-width:2px
    style Output fill:#ff9,stroke:#444,stroke-width:2px
```
---

## ğŸ—ï¸ System Architecture

### 3.1 Document Ingestion & Chunking

- **Tool:** `unstructured.partition.pdf` with custom heuristics  
- **Custom Logic:**
  - Detect Title and Table elements
  - Skip short captions before tables
  - Flush and group non-table elements into sections
  - Label orphan text as `"Untitled Section"`

- **Output:**  
  - JSON chunks with `section_title`, `type` (text/table), and `raw_text`  
  - Saved under `docs_chunks/`  
  - Logged in `chunking.log`

---

### 3.2 Embedding & Vector Store

- **Embeddings Model:** `text-embedding-3-small` (1536-dim)
- **Embedding Script:** Located in `src/ingest`

  **Process:**
  - Reads JSON from `docs_chunks/`
  - Generates embeddings using OpenAI API
  - Formats payload with metadata
  - Writes formatted data to `qdrant_ready_embeddings/`

- **Qdrant Upload:**
  - Connects to local Docker-hosted Qdrant instance
  - Ensures collection `policy_chunks` exists (COSINE distance)
  - Uploads as `PointStruct` objects with:
    - UUID5-based `id`
    - 1536-dim `vector`
    - Associated `payload`

---

### 3.3 Retriever

- **Function:** `retrieve_relevant_chunks(query, top_k)`
  
  **Process:**
  - Computes embedding of input query
  - Searches Qdrant with `hnsw_ef=128`
  - Returns top-k most relevant text chunks for RAG-based generation

---

### 3.4 Employee Metadata Loader

- **Utility:** `load_employee_metadata(name)`

  **Features:**
  - Reads from `Employee_List.json`
  - Normalizes fields to keys:
    - `name`, `team`, `band`, `base_salary`, etc.
  - Raises descriptive errors if:
    - File is missing
    - Employee is not found

---

### 3.5 Offer Letter Generation

#### âœ… Primary Generator: `generate_offer_letter.py`

- Loads employee metadata and retrieved policy chunks
- Constructs strict `system` and `user` prompts
- Calls `gpt-4o-mini` with low temperature for deterministic output
- Verifies presence of candidate name and letter length
- Fallback to Jinja2 templating if generation fails

#### ğŸ”„ No-RAG Variant: `generate_offer_withoutrag.py`

- Skips retrieval stage
- Generates letter using metadata-only prompt
- Enforces strict formatting in GPT prompt

#### ğŸ§° Jinja2 Fallback: `generate_offer_letter_jinja(emp, chunks?)`

- Uses `offer_template.txt` from `templates/`
- Injects:
  - Candidate metadata
  - `TITLE_BY_TEAM` and `WFO_POLICY_BY_TEAM` lookups
- Applies `comma` filter for formatting salary values

---

### 3.6 API Layer

- **Framework:** FastAPI (`api_server.py`)

  **Endpoints:**
  - `GET /` â€“ Health check
  - `POST /generate-offer-letter`  
    - Accepts: `employee_name`, `use_jinja` flag  
    - Routes to GPT or Jinja2 generator  
    - Returns JSON with:
      - Generation `status`
      - `source` (GPT or Jinja2)
      - Generated `letter_text`

- CORS enabled for frontend access

---

### 3.7 Frontend Interfaces

#### ğŸŒ Static HTML Chatbot (`index.html`)

- Simple HTML + JS
- Sends generation requests to FastAPI (directly or via `ngrok`)

#### ğŸ“º Streamlit App (`app.py`)

- Toggle: GPT-based or template-based generation
- Input: Dropdown or text box for employee selection
- Preview: Compensation table preview
- Output: Multi-line display of generated letter
- Export: PDF download via `save_offer_letter_pdf`
- Tracks session-based generation history

---

### ğŸš€ Deployment

- **Backend:**  
  - Hosted on **Render.com**  
  - Uses Docker + Uvicorn for FastAPI server

- **Frontend:**  
  - Static site hosted on **Vercel**  
  - Streamlit app hosted on **Hugging Face Spaces**

---
## ğŸ”„ Complete Workflow

---

### 1. Preprocessing & Chunking

```mermaid
graph LR
    A[ğŸ“„ PDF Files in data/] --> B[ğŸ§± Chunking with unstructured]
    B --> C[ğŸ“¦ JSON chunks in docs_chunks/]
    C --> D[ğŸ”¢ OpenAI Embeddings]
    D --> E[ğŸ“‹ Qdrant-ready format]
```

**Steps:**

- ğŸ“„ Input PDFs â†’ `data/` (HR policies, sample letters)  
- ğŸ§± Chunking â†’ `unstructured` library â†’ `docs_chunks/`  
- ğŸ”¢ Embeddings â†’ OpenAI `text-embedding-3-small` â†’ `embeddings/`  
- ğŸ“¦ Qdrant formatting â†’ `qdrant_ready_embeddings/`

---

### 2. Vector Store Setup

```bash
# Start Qdrant instance
docker run -p 6333:6333 qdrant/qdrant

# Upload embeddings
python src/ingest/embed_and_upload.py
```

- ğŸ³ Qdrant instance (via Docker) running locally  
- ğŸ” Upload embeddings using upload scripts  
- ğŸ” Built retriever using Qdrant client to fetch relevant chunks  

---

### 3. Employee Metadata Handling

```mermaid
graph LR
    A[ğŸ“„ Employee_List.csv] --> B[ğŸ”„ CSV to JSON conversion]
    B --> C[ğŸ“¥ Load via load_employee_metadata.py]
    C --> D[âœ… Normalized employee data]
```

- ğŸ“„ `Employee_List.csv` â†’ converted to `Employee_List.json`  
- ğŸ“¥ Loaded via `load_employee_metadata.py`  
- ğŸ—ºï¸ Role-wise policy mapping via `wfo_policy.json`

---

### 4. Offer Letter Generation Pipeline

```mermaid
graph TD
    A[ğŸ” Employee Name Input] --> B[ğŸ“Š Load Metadata]
    B --> C[ğŸ” RAG Retrieval]
    C --> D[ğŸ¤– LLM Generation]
    D --> E{âœ… Success?}
    E -->|Yes| F[ğŸ“„ Generated Letter]
    E -->|No| G[ğŸ”§ Jinja2 Fallback]
    G --> F
    F --> H[ğŸ“‹ PDF Export]
```

#### Generation Modes:

##### ğŸ¤– LLM + RAG-based (via `generate_offer_letter.py`):
- Retrieves relevant policy chunks
- Combines with employee metadata
- Uses `gpt-4o-mini` for natural language generation

##### âœ‚ï¸ No-RAG (via `generate_offer_withoutrag.py`):
- LLM-only generation without retrieval
- Uses only employee metadata

##### ğŸ§¾ Jinja2 Fallback (via `generate_offer_letter_nollm.py`):
- Template-based generation using `templates/`
- Deterministic output on LLM failure

---

### 5. Frontend & API Integration

```mermaid
graph LR
    A[ğŸŒ Streamlit Frontend] --> B[âš¡ FastAPI Backend]
    B --> C[ğŸ¤– Generation Engine]
    C --> D[ğŸ“‹ PDF Output]
    
    E[ğŸ–¥ï¸ HTML Interface] --> B
    F[ğŸ“± Mobile Interface] --> B
```

- ğŸ–¥ï¸ `api_server.py` â†’ Exposes REST endpoints  
- ğŸŒ `index.html` â†’ Chat-style web interface  
- ğŸ“¡ Backend hosted on **Render**, frontend on **Vercel**  
- ğŸ§ª Streamlit app hosted on **Hugging Face Spaces**

---

### 6. Additional Features

- ğŸ”„ Frontend toggle for switching generation modes  
- ğŸ“ PDF saving via `save_offer_letter_pdf.py`  
- ğŸ“œ Font styling using `DejaVuSans.ttf`  
- ğŸ“Š Compensation preview tables  
- ğŸ“ˆ Generation history tracking

---

## ğŸš€ Getting Started

### âœ… Prerequisites

- Python 3.9+
- Docker (for Qdrant)
- OpenAI API key

---

### ğŸ› ï¸ Environment Setup

Create a `.env` file in the project root:

```env
OPENAI_API_KEY=your_openai_api_key_here
QDRANT_URL=http://localhost:6333
QDRANT_COLLECTION=policy_chunks
```

---

### ğŸ“¦ Installation

```bash
# Clone repository
git clone <your-repo-url>
cd offer-letter-generator

# Install dependencies
pip install -r requirements.txt

# Start Qdrant vector database
docker run -p 6333:6333 qdrant/qdrant
```

---

### ğŸ“„ Data Preprocessing

```bash
# Chunk documents
python src/ingest/chunk_docs.py

# Generate embeddings and upload to Qdrant
python src/ingest/embed_and_upload.py

# Convert employee CSV to JSON (if needed)
python csv_to_json.py
```

---

### ğŸš€ Running the Application

#### Option 1: API Server + Static Frontend

```bash
# Start FastAPI backend
uvicorn api_server:app --reload

# Serve static frontend (in another terminal)
python3 -m http.server 5500
# Open http://localhost:5500 in browser
```

#### Option 2: Streamlit Application

```bash
# Start Streamlit app
streamlit run app.py
```

#### Option 3: Direct Script Usage

```bash
# Generate offer letter using LLM + RAG
python generate_offer_letter.py --employee "John Doe"

# Use Jinja2 template fallback
python generate_offer_letter_nollm.py --employee "John Doe"
```

## ğŸ“¡ API Endpoints

### ğŸ” Health Check

```http
GET /
```

### ğŸ“¨ Generate Offer Letter

```http
POST /generate-offer-letter
Content-Type: application/json
```

#### Request Body

```json
{
    "employee_name": "John Doe",
    "use_jinja": false
}
```

#### Response

```json
{
    "status": "success",
    "source": "llm_rag",
    "letter": "Dear John Doe,\n\nWe are pleased to offer you..."
}
```

---

## ğŸ› ï¸ Configuration Files

### ğŸ“‹ Employee Metadata (`Employee_List.json`)

```json
{
    "John Doe": {
        "name": "John Doe",
        "team": "Engineering",
        "band": "Senior",
        "base_salary": 120000,
        "bonus": 15000,
        "benefits": 8000
    }
}
```

### ğŸ—ºï¸ WFO Policy Mapping (`wfo_policy.json`)

```json
{
  "Engineering": {
    "MinDays": "3/week",
    "SuggestedDays": "Mon, Tue, Thu",
    "RemoteNotes": "Sprint reviews must be in-office"
  },
```

---

## ğŸ”§ Troubleshooting

### âš ï¸ Common Issues

#### âŒ Qdrant Connection Error

```bash
# Ensure Qdrant is running
docker ps

# Restart if needed
docker run -p 6333:6333 qdrant/qdrant
```

#### âŒ OpenAI API Errors

- Check API key in `.env` file  
- Verify API quota and billing on OpenAI dashboard  

#### âŒ Missing Employee Data

- Ensure employee exists in `Employee_List.json`  
- Check name spelling and formatting  

#### âŒ PDF Generation Issues

- Verify `DejaVuSans.ttf` font file exists  
- Check write permissions for `offer_letters/` directory  

---

## ğŸš¢ Deployment

### ğŸ§© Backend (Render.com)

- Connect GitHub repository  
- Set environment variables  
- Deploy with Docker  

### ğŸŒ Frontend (Vercel)

- Import repository  
- Set build command: `npm run build` (if applicable)  
- Deploy static files  

### ğŸ“º Streamlit App (Hugging Face Spaces)

- Create new Space  
- Upload `app.py` and dependencies  
- Configure secrets for API keys  

---

## ğŸ”® Future Enhancements

- ğŸ¯ **Dynamic policy filters:** Filter retrieved chunks by department or band  
- ğŸ“§ **Bulk generation:** Batch process all employees and email letters via SMTP  
- ğŸŒ **Internationalization:** Support multi-language templates  
- ğŸ‘” **Role-based templates:** Different Jinja templates per job family  
- ğŸ’» **CLI tool:** `offergen --name "Alice" --mode template` for scripting  
- ğŸ“Š **Analytics dashboard:** Track generation metrics and success rates  
- ğŸ” **Authentication:** Add user management and role-based access  


